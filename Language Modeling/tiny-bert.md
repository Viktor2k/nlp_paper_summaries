# TLDR - TinyBERT
Knowledge distillation of transformer based language models is more relevant than ever since it, in many cases, is what enables usage of such models in production environments. [TinyBERT](https://arxiv.org/abs/1909.10351) introduces a tailored framework which enables a small student model to learn to mimic a, preferably, larger teacher's activations at the embedding, attention and prediction layers. The way the *loss functions* are formulated allow for full freedom when selecting size of the student, which in previous work was constrained by the teacher dimensions.

As should be known by anyone familiar with transfer learning does training often happen in two distinct steps; a general pre-training followed by a task specific fine tuning. The same holds true in the distillation of TinyBERT, where the approach differs most during fine-tuning. Initially, a general distillation is performed with a pre-trained, general, teacher model. It is followed bu possible to fine-tune the student using an already fine-tuned teacher on task specific data. To get the most out of this specialised teacher's activations did the authors augment their training data, essentially bootstrapping its size.

These two contributions enabled the authors to achieve 96% of a BERT base model's performance (110M params) with TinyBERT equivalent in size to BERT small (15M parameters). This is a 7x difference in size and 9x difference in inference speed! Comparing a vanilla BERT small to TinyBERT reveal a 6.3 points improvement on GLUE with the distilled model, a hefty 9% improvement!